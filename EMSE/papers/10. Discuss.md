## [[10. (discuss) Mueller_ICSE_2016.pdf]]
* **Summary**: Using biometrics to assess code difficulty.
* **Problem**: Lack of tooling to automatically assess code difficulty.
* **Results**
	* How do biometrics compare to traditional metrics of code quality? outperform.
	* Generalise to other companies? one-week replication study.
### Introduction
tech debt/bad code generate costs.
code reviews help, but require time and effort.
previous automated approaches insufficient, as they
- require extra information (like code history).
- do not take into account differences in code comprehension between devs.
HRV/EDA linked with task difficulty.
study: use biometrics to find code which is perceived as difficult with 10 devs in Canadian company.
results
- biometrics outperform traditional metrics and naive classifier.
- code elements perceived as more difficult contain more peer review concerns.
- helps automatically detect 50% of bugs found in code reviews.
generalisability: 1-week replication study with five devs in Swiss company. Some findings can be replicated.
### Context: Related Work and Psychological Background
1. manual detection of quality concerns.
	- lightweight code reviews provide substantial benefits.
	- require time and effort by peer developers.
2. automatic detection based on traditional metrics.
	* complexity, size, change are common.
	* oragnisational information to predict (how many devs touched a file, for example)
	* FindBugs, PMD.
	* defect prediction can be improved by micro interaction metrics (for example, ratio between edits and selects)
	* interaction logs of IDE to predict difficulty by dev.
3. use of biometrics in software dev.
	* skin, heart, breathing mainly.
	* have been used to assess mental load in software eng.
	* predict difficulty of code comprehension
cognitive load theory: intrinsic (inherent task difficulty), extrinsic (for example, code quality), germane load (effort for processing information)
more difficult task > higher cognitive load, worse performance.
biometrics can be used to approximate cognitive load.
### Methods
RQs
- biometrics to identify difficult code?
- biometrics to identify quality concerns found in peer reviews?
- biometrics compared to traditional metrics?
- sensitivity to the individual?
**Participants**: 23 to 45 years old, 3 to 22 years of experience, same project, different teams, agile. could access their biometric data and quit anytime.
**Sensors**: wristband for skin and heart, chest strap for skin, heart and breathing. wristband was optional (6/10 wore it).
**Procedure**: asked to install interaction monitoring plugin in Eclipse. put on sensors, charge after each day. continue work as regular. baseline by watching video of fish swimming in fish tank for normalisation. eclipse plugin asks each 90mins to rate difficulty of elements currently being worked with. Also on commits.
**Metrics**: biometrics, code metrics, interaction metrics, change metrics.
Outcome measures: perceived difficulty during change task (survey every 90mins in IDE), perceived difficulty at end of change task (survey after commit in IDE), code quality concerns via peer reviews (results of code reviews collected).
**Data Collection**: 116 developer work days. 1500 methods and 1500 classes were rated in perceived difficulty.
### Results: Analysis and Replication Study

### Discussion

### Threats to validity

### Conclusion

# Discussion Points
- no pull request workflow, just commits? is this realistic?
- Hawthorne effect: change in behavior under observation.
- other factors: sleep, caffeine, exercise, â€¦
- is high cognitive load really a proxy for poor code quality? developer might be intently focused or stressed by outside factors.
- physiological response lag (different metrics would have different lag, not just 2 seconds accounted)
- systematic bias in self-assessing difficulty (confidence, perfectionism, work ethic). not addressed beyond acknowleding subjectivity.