{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "In this exercise, your task is to classify table data using a Decision Tree and a Random Forest classifiers.\n",
    "\n",
    "You have two datasets: `iris_2D.csv` and `adult_onehotencoded.csv`.\n",
    "\n",
    "`iris_2D.csv` contains three different types of irises' and their petal and sepal sizes.\n",
    "\n",
    "`adult_onehotencoded.csv` contains two groups of income: <=50K and >50K and other features such as education, gender, occupation, etc.\n",
    "\n",
    "You have to implement the following:\n",
    "\n",
    "1. Decision Tree classifier using sklearn with the arguments (see function's signature) and random_state=42;\n",
    "\n",
    "1. Pipeline that splits data into 80\\% of training and 20\\% of test data with shuffling and random_state=42, trains a Decision Tree classifier, and predicts the labels for the test data;\n",
    "\n",
    "1. Same as 1 & 2 but for a Random Forest classifier;\n",
    "\n",
    "4. Top-N features of a classifier by their importance.\n",
    "\n",
    "Note: always set your random_state to make your results reproducible. However, do not fix it in production as it prevents your model from being exposed to the natural randomness of new data, potentially leading to overconfident or less reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import Optional\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table_dataset(\n",
    "    dataset_path: str,\n",
    ") -> tuple[ArrayLike, ArrayLike, list[str]]:\n",
    "    \"\"\"Load a dataset from a CSV file using pandas.\n",
    "    The last column is treated as the target variable, and all other columns are treated as features.\n",
    "    Args:\n",
    "        dataset_path (str): Path to the CSV file.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X (ArrayLike): Feature matrix.\n",
    "            - y (ArrayLike): Target vector.\n",
    "            - feature_names (list[str]): List of feature names.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    if \"fnlwgt\" in df.columns:\n",
    "        df.drop(columns=[\"fnlwgt\"], inplace=True)\n",
    "\n",
    "    X = df.iloc[:, :-1].to_numpy()\n",
    "    y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "    feature_names = df.iloc[:, :-1].columns.tolist()\n",
    "\n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_kfold(\n",
    "    classifier: ClassifierMixin, X: ArrayLike, y: ArrayLike, k: int = 10\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Perform k-fold cross-validation on the given classifier and dataset.\n",
    "    Args:\n",
    "        classifier: The classifier to evaluate.\n",
    "        X (ArrayLike): Feature matrix.\n",
    "        y (ArrayLike): Target vector.\n",
    "        k (int): Number of folds for cross-validation.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - mean_train_score (float): Mean training score across all folds.\n",
    "            - mean_val_score (float): Mean validation score across all folds.\n",
    "    \"\"\"\n",
    "    stratified_shuffled_kfold = StratifiedShuffleSplit(\n",
    "        n_splits=k, test_size=1.0 / k, random_state=42\n",
    "    )\n",
    "\n",
    "    val_scores = np.empty((k,))\n",
    "    train_scores = np.empty((k,))\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(stratified_shuffled_kfold.split(X, y)):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_val_pred = classifier.predict(X_val)\n",
    "        y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "        val_scores[i] = accuracy_score(y_val, y_val_pred)\n",
    "        train_scores[i] = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    mean_val_score = np.mean(val_scores)\n",
    "    mean_train_score = np.mean(train_scores)\n",
    "\n",
    "    return mean_train_score, mean_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_curve_data(\n",
    "    X: ArrayLike,\n",
    "    y: ArrayLike,\n",
    "    classifier: ClassifierMixin,\n",
    "    hyperparam_name: str,\n",
    "    hyperparam_values: any,\n",
    ") -> tuple[int, ArrayLike, ArrayLike]:\n",
    "    \"\"\"Test and plot the performance of a classifier with different hyperparameter values.\n",
    "    Args:\n",
    "        X (ArrayLike): Feature matrix.\n",
    "        y (ArrayLike): Target vector.\n",
    "        classifier: The classifier to evaluate.\n",
    "        hyperparam_name (str): Name of the hyperparameter to test.\n",
    "        hyperparam_values (any): Values of the hyperparameter to test.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - best_hyperparam_value (int): The best hyperparameter value.\n",
    "            - train_scores (ArrayLike): Training scores for each hyperparameter value.\n",
    "            - val_scores (ArrayLike): Validation scores for each hyperparameter value.\n",
    "    \"\"\"\n",
    "\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    for val in hyperparam_values:\n",
    "        print(f\"Testing {hyperparam_name} = {val}\")\n",
    "        classifier.set_params(**{hyperparam_name: val})\n",
    "        train_score, val_score = stratified_kfold(classifier=classifier, X=X, y=y, k=5)\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "\n",
    "    return np.argmax(val_scores).item(), train_scores, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_curve(\n",
    "    hyperparam_name: str,\n",
    "    hyperparam_values: ArrayLike,\n",
    "    train_scores: ArrayLike,\n",
    "    val_scores: Optional[ArrayLike] = None,\n",
    "    score_name: str = \"Accuracy\",\n",
    ") -> None:\n",
    "    \"\"\"Plots the score curve.\n",
    "    Args:\n",
    "        hyperparam_name (str): Name of the hyperparameter.\n",
    "        hyperparam_values (ArrayLike): Values of the hyperparameter.\n",
    "        train_scores (ArrayLike): Training scores for each hyperparameter value.\n",
    "        val_scores (Optional[ArrayLike]): Validation scores for each hyperparameter value.\n",
    "        score_name (str): Name of the score metric. Default is \"Accuracy\".\n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(\n",
    "        hyperparam_values,\n",
    "        train_scores,\n",
    "        color=\"#D81B60\",\n",
    "        linewidth=2.5,\n",
    "        label=f\"Train {score_name}\",\n",
    "    )\n",
    "    if val_scores is not None:\n",
    "        plt.plot(\n",
    "            hyperparam_values,\n",
    "            val_scores,\n",
    "            color=\"#1E88E5\",\n",
    "            linewidth=2.5,\n",
    "            label=f\"Valid {score_name}\",\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.xlabel(hyperparam_name)\n",
    "    plt.ylabel(f\"{score_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_classifier(\n",
    "    X: ArrayLike,\n",
    "    y: ArrayLike,\n",
    "    max_depth: int = None,\n",
    "    min_samples_split: int = 2,\n",
    "    min_samples_leaf: int = 1,\n",
    ") -> DecisionTreeClassifier:\n",
    "    \"\"\"Create and train a decision tree classifier.\n",
    "    Args:\n",
    "        X (ArrayLike): Feature matrix.\n",
    "        y (ArrayLike): Target vector.\n",
    "        max_depth (int): Maximum depth of the tree.\n",
    "        min_samples_split (int): Minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf (int): Minimum number of samples required to be at a leaf node.\n",
    "    Returns:\n",
    "        DecisionTreeClassifier: Trained decision tree classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement a Decision Tree classifier using sklearn\n",
    "    # Read the task description!\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_decision_tree_classifier(\n",
    "    X: ArrayLike,\n",
    "    y: ArrayLike,\n",
    "    max_depth: int = None,\n",
    "    min_samples_split: int = 2,\n",
    "    min_samples_leaf: int = 1,\n",
    ") -> DecisionTreeClassifier:\n",
    "    \"\"\"Create a pipeline for table data classification using a decision tree classifier.\n",
    "    Args:\n",
    "        X (ArrayLike): Feature matrix.\n",
    "        y (ArrayLike): Target vector.\n",
    "        max_depth (int): Maximum depth of the tree.\n",
    "        min_samples_split (int): Minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf (int): Minimum number of samples required to be at a leaf node.\n",
    "    Returns:\n",
    "        DecisionTreeClassifier: Trained decision tree classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement the pipeline using the above implemented functions\n",
    "    # Read the task description!\n",
    "\n",
    "    ...\n",
    "\n",
    "    print(f\"Accuracy score: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names = load_table_dataset(\"iris_2D.csv\")\n",
    "\n",
    "clf = pipeline_decision_tree_classifier(\n",
    "    X, y, max_depth=None, min_samples_split=20, min_samples_leaf=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(16, 9))\n",
    "plot_tree(clf, ax=ax, feature_names=feature_names, filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(\n",
    "    X: ArrayLike,\n",
    "    y: ArrayLike,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = None,\n",
    "    min_samples_split: int = 2,\n",
    "    min_samples_leaf: int = 1,\n",
    ") -> RandomForestClassifier:\n",
    "    \"\"\"Create and train a decision tree classifier.\n",
    "    Args:\n",
    "        X (ArrayLike): Feature matrix.\n",
    "        y (ArrayLike): Target vector.\n",
    "        n_estimators (int): Number of trees in the forest.\n",
    "        max_depth (int): Maximum depth of the tree.\n",
    "        min_samples_split (int): Minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf (int): Minimum number of samples required to be at a leaf node.\n",
    "    Returns:\n",
    "        RandomForestClassifier: Trained random forest classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement a Random Forest classifier using sklearn with the following arguments (see function's signature)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_random_forest_classifier(\n",
    "    X: ArrayLike,\n",
    "    y: ArrayLike,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = None,\n",
    "    min_samples_split: int = 2,\n",
    "    min_samples_leaf: int = 1,\n",
    ") -> RandomForestClassifier:\n",
    "    \"\"\"Create a pipeline for table data classification using a decision tree classifier.\n",
    "    Args:\n",
    "        X (ArrayLike): Feature matrix.\n",
    "        y (ArrayLike): Target vector.\n",
    "        max_depth (int): Maximum depth of the tree.\n",
    "        min_samples_split (int): Minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf (int): Minimum number of samples required to be at a leaf node.\n",
    "    Returns:\n",
    "        RandomForestClassifier: Trained random forest classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement the pipeline using the above implemented functions\n",
    "    # Read the task description!\n",
    "\n",
    "    ...\n",
    "\n",
    "    print(f\"Accuracy score: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names = load_table_dataset(\"iris_2D.csv\")\n",
    "\n",
    "clf = pipeline_random_forest_classifier(\n",
    "    X,\n",
    "    y,\n",
    "    n_estimators=10,\n",
    "    max_depth=None,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names = load_table_dataset(\"adult_onehotencoded.csv\")\n",
    "\n",
    "clf = pipeline_decision_tree_classifier(\n",
    "    X, y, max_depth=5, min_samples_split=2, min_samples_leaf=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_name = \"max_depth\"\n",
    "hyperparam_values = np.arange(1, 11)\n",
    "\n",
    "best_hyperparam_value, train_scores, val_scores = get_score_curve_data(\n",
    "    X, y, clf, hyperparam_name, hyperparam_values\n",
    ")\n",
    "\n",
    "print(f\"Hyperparameter name: {hyperparam_name}\")\n",
    "print(f\"Best hyperparameter value: {hyperparam_values[best_hyperparam_value]}\")\n",
    "\n",
    "plot_score_curve(\n",
    "    hyperparam_name,\n",
    "    hyperparam_values,\n",
    "    train_scores,\n",
    "    val_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "1. What is the best `max_depth` for the Decision Tree Classifier on this dataset? (All other parameters stay fixed)\n",
    "2. What other parameters can you tweak for the Decision Tree Classifier except `max_depth`?\n",
    "3. What is the best set of parameters for the Decision Tree Classifier for this dataset?\n",
    "    - Note: you could use grid search to figure it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names = load_table_dataset(\"adult_onehotencoded.csv\")\n",
    "\n",
    "clf = pipeline_random_forest_classifier(\n",
    "    X,\n",
    "    y,\n",
    "    n_estimators=10,\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_name = \"n_estimators\"\n",
    "hyperparam_values = np.arange(1, 11)\n",
    "\n",
    "best_hyperparam_value, train_scores, val_scores = get_score_curve_data(\n",
    "    X, y, clf, hyperparam_name, hyperparam_values\n",
    ")\n",
    "\n",
    "print(f\"Hyperparameter name: {hyperparam_name}\")\n",
    "print(f\"Best hyperparameter value: {hyperparam_values[best_hyperparam_value]}\")\n",
    "\n",
    "plot_score_curve(\n",
    "    hyperparam_name,\n",
    "    hyperparam_values,\n",
    "    train_scores,\n",
    "    val_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "1. What is the best `n_estimators` for the Random Forest Classifier on this dataset? (All other parameters stay fixed)\n",
    "    - Note: this may take several minutes to run.\n",
    "2. What other parameters can you tweak for the Random Forest Classifier except `n_estimators`?\n",
    "3. What is the best set of parameters for the Random Forest Classifier for this dataset?\n",
    "    - Note: you could use grid search to figure it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topn_features(\n",
    "    classifier: DecisionTreeClassifier | RandomForestClassifier,\n",
    "    feature_names: list[str],\n",
    "    n: int = 10,\n",
    ") -> tuple[list[str], ArrayLike]:\n",
    "    \"\"\"Get the top n features based on feature importances.\n",
    "    Args:\n",
    "        classifier (DecisionTreeClassifier | RandomForestClassifier): Trained classifier.\n",
    "        feature_names (list[str]): List of feature names.\n",
    "        n (int): Number of top features to retrieve.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - topn_feat_names (list[str]): List of top n feature names.\n",
    "            - topn_feat_importances (ArrayLike): Array of top n feature importances.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Get the top n features of the classifier based on their importance\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topn_features(\n",
    "    topn_feat_names: list[str],\n",
    "    topn_feat_importances: ArrayLike,\n",
    "    n: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"Plot the top n features of a classifier.\n",
    "    Args:\n",
    "        clf (DecisionTreeClassifier | RandomForestClassifier): The classifier to evaluate.\n",
    "        feature_names (list[str]): List of feature names.\n",
    "        n (int): Number of top features to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(\n",
    "        y=topn_feat_names,\n",
    "        x=topn_feat_importances * 100,\n",
    "        orient=\"h\",\n",
    "        color=\"#1E88E5\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Relative feature importance / %\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "\n",
    "    fig.suptitle(\"Feature importance to predict income >= 50K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline_random_forest_classifier(\n",
    "    X,\n",
    "    y,\n",
    "    n_estimators=10,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    ")\n",
    "\n",
    "n = 10\n",
    "topn_feat_names, topn_feat_importances = get_topn_features(clf, feature_names, n)\n",
    "\n",
    "plot_topn_features(topn_feat_names, topn_feat_importances, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "1. What features are in the top-10 most correlated with income >= 50K?\n",
    "1. Is your dataset balanced?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
