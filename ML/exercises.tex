\documentclass{article}

\usepackage[solutions]{xrcise}

\subject{Machine Learning}
\semester{Summer 2025}
\author{Leopold Lemmermann}

\begin{document}
\createtitle

\sheet[0]{Repeat Linear Algebra}

\begin{exercise}{Norms}
  Consider the following vectors:
  \[
    \mathbf{w}_1 = (0,\,1,\,2,\,3)^\top, 
    \quad 
    \mathbf{w}_2 = (2,\,3,\,0,\,0)^\top.
  \]
  Compute the 0-norm, 1-norm, 2-norm (Euclidean norm), and the $\infty$-norm of each vector.
  
  \begin{solution}
    For $\mathbf{w}_1 = (0,\,1,\,2,\,3)$:
    \[
      \|\mathbf{w}_1\|_0 = 3,\quad
      \|\mathbf{w}_1\|_1 = |0| + |1| + |2| + |3| = 6,\quad
      \|\mathbf{w}_1\|_2 = \sqrt{0^2 + 1^2 + 2^2 + 3^2} = \sqrt{14},\quad
      \|\mathbf{w}_1\|_\infty = 3.
    \]

    For $\mathbf{w}_2 = (2,\,3,\,0,\,0)$:
    \[
      \|\mathbf{w}_2\|_0 = 2,\quad
      \|\mathbf{w}_2\|_1 = |2| + |3| + |0| + |0| = 5,\quad
      \|\mathbf{w}_2\|_2 = \sqrt{2^2 + 3^2} = \sqrt{13},\quad
      \|\mathbf{w}_2\|_\infty = 3.
    \]
  \end{solution}
\end{exercise}

\begin{exercise}{Determinant, Trace, Inverse}
  Compute the determinant, the trace, and the inverse of each matrix:
  \[
    A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix},
    \quad
    A_2 = \begin{pmatrix} 2 & 10 \\ 0 & 100 \end{pmatrix}.
  \]
  
  \begin{solution}
    For 
    \[
      A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix}:
      \quad
      \det(A_1) = 1 \cdot 100 = 100, 
      \quad
      \mathrm{trace}(A_1) = 101, 
      \quad
      A_1^{-1} = \begin{pmatrix} 1 & 0 \\[4pt] 0 & \tfrac{1}{100} \end{pmatrix}.
    \]

    For 
    \[
      A_2 = \begin{pmatrix} 2 & 10 \\ 0 & 100 \end{pmatrix}:
      \quad
      \det(A_2) = 2 \cdot 100 - 0 \cdot 10 = 200,
      \quad
      \mathrm{trace}(A_2) = 102,
    \]
    \[
      A_2^{-1}
      = \frac{1}{200} 
      \begin{pmatrix}
        100 & -10 \\[3pt]
        0   & 2
      \end{pmatrix}
      = \begin{pmatrix}
        \tfrac{1}{2} & -\tfrac{1}{20} \\[3pt]
        0            & \tfrac{1}{100}
      \end{pmatrix}.
    \]
  \end{solution}
\end{exercise}

\begin{exercise}{Eigenvalues and Eigenvectors}
  Compute the eigenvalues and corresponding eigenvectors of:
  \[
    A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix},
    \quad
    A_2 = \begin{pmatrix} 2 & 10 \\ 10 & 2 \end{pmatrix}.
  \]
  
  \begin{solution}
    For 
    \[
      A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix}:
      \quad
      \lambda_1 = 1,\; \lambda_2 = 100.
    \]
    An eigenvector for $\lambda_1=1$ is $(1,\,0)^\top$, and for $\lambda_2=100$ is $(0,\,1)^\top$.

    For 
    \[
      A_2 = \begin{pmatrix} 2 & 10 \\ 10 & 2 \end{pmatrix}:
    \]
    Solve $\det(A_2 - \lambda I)=0$:
    \[
      (2 - \lambda)^2 - 10\cdot10 = 0
      \;\Rightarrow\;
      (2-\lambda)^2 = 100
      \;\Rightarrow\;
      2-\lambda = \pm 10
      \;\Rightarrow\;
      \lambda_1 = 12,\;\lambda_2 = -8.
    \]
    An eigenvector for $\lambda_1=12$ is $(1,\,1)^\top$, and for $\lambda_2=-8$ is $(1,\,-1)^\top$.
  \end{solution}
\end{exercise}

\begin{exercise}{Definiteness}
  Classify the following matrices as positive semidefinite, positive definite, negative semidefinite, negative definite, or indefinite:
  \[
    A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix},
    \quad
    A_2 = \begin{pmatrix} 2 & 10 \\ 10 & 2 \end{pmatrix}.
  \]
  
  \begin{solution}
    The matrix 
    \[
      A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix}
    \]
    has eigenvalues $1$ and $100$, both $> 0$, so it is positive definite.

    The matrix 
    \[
      A_2 = \begin{pmatrix} 2 & 10 \\ 10 & 2 \end{pmatrix}
    \]
    has eigenvalues $12$ and $-8$, one positive and one negative, so it is indefinite.
  \end{solution}
\end{exercise}

\begin{exercise}{Rank}
  Determine the rank of:
  \[
    A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix},
    \quad
    A_2 = \begin{pmatrix} 2 & 10 \\ 10 & 50 \end{pmatrix}.
  \]
  
  \begin{solution}
    The matrix 
    \[
      A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 100 \end{pmatrix}
    \]
    is invertible (nonzero determinant), so its rank is 2.

    For 
    \[
      A_2 = \begin{pmatrix} 2 & 10 \\ 10 & 50 \end{pmatrix}:
    \]
    \[
      \det(A_2) = 2\cdot50 - 10\cdot10 = 100 - 100 = 0,
    \]
    and the second row is 5 times the first, so its rank is 1.
  \end{solution}
\end{exercise}

\begin{exercise}{Linear System}
  Solve the system:
  \[
    \begin{pmatrix}
      1 & 1 & 1 \\
      1 & 2 & 3 \\
      2 & 3 & 4
    \end{pmatrix}
    \mathbf{x}
    =
    \begin{pmatrix}
      1 \\ 2 \\ 3
    \end{pmatrix}.
  \]
  
  \begin{solution}
    Let $\mathbf{x} = (x_1,\,x_2,\,x_3)^\top$. Then:
    \[
      \begin{cases}
        x_1 + x_2 + x_3 = 1,\\
        x_1 + 2x_2 + 3x_3 = 2,\\
        2x_1 + 3x_2 + 4x_3 = 3.
      \end{cases}
    \]
    Subtracting the first equation from the second gives $x_2 + 2x_3 = 1.$ 
    An appropriate combination for the third yields the same constraint. 
    Thus $x_2 = 1 - 2x_3.$ 
    From the first equation, $x_1 + (1 - 2x_3) + x_3 = 1,$ which simplifies to $x_1 = x_3.$ 
    Letting $x_3 = t$ gives
    \[
      \mathbf{x} = 
      \begin{pmatrix}
        t \\[3pt]
        1 - 2t \\[3pt]
        t
      \end{pmatrix}
      \quad (t \in \mathbb{R}).
    \]
  \end{solution}
\end{exercise}

\begin{exercise}{Inverse of a Transpose}
  Prove that $(A^\top)^{-1} = (A^{-1})^\top$.
  
  \begin{solution}
    From $A\,A^{-1} = I,$ taking transposes gives $(A^{-1})^\top \,A^\top = I.$ 
    By definition of inverse, $(A^\top)^{-1}$ is the unique matrix $B$ satisfying $A^\top B = I,$ 
    so $B = (A^{-1})^\top.$ 
    Hence $(A^\top)^{-1} = (A^{-1})^\top.$
  \end{solution}
\end{exercise}

\sheet[1]{Least Squares Regression}

\begin{exercise}{Interpretation of Weights and Offset}
  How do you interpret the weights $\mathbf{w}$ and offset $b$ in a least squares regression analysis?
  
  \begin{solution}
    In linear regression, $\mathbf{w}$ indicates how each input feature influences the prediction, 
    while $b$ (often called intercept) is a constant shift. 
    For one-dimensional inputs, these represent slope and intercept of a best-fit line; 
    in higher dimensions, they define the best-fit hyperplane.
  \end{solution}
\end{exercise}

\begin{exercise}{Gradient of the Least Squares Loss}
  Show that 
  \[
    \nabla_{\mathbf{w}} \bigl\|\mathbf{y} - X\,\mathbf{w}\bigr\|_2^2 
    \;=\; 
    2\,X^\top\!\bigl(X\,\mathbf{w} - \mathbf{y}\bigr).
  \]
  (You may omit matrix calculus and verify by computing partial derivatives w.r.t.\ each $w_i$.)
  
  \begin{solution}
    Writing $\|\mathbf{y} - X\,\mathbf{w}\|_2^2 = \sum_{j=1}^n \bigl(y_j - \mathbf{x}_j^\top \mathbf{w}\bigr)^2,$ 
    with $\mathbf{x}_j^\top$ as the $j$th row of $X$, each partial derivative w.r.t.\ $w_i$ involves
    \[
      \frac{\partial}{\partial w_i}\bigl(\mathbf{x}_j^\top \mathbf{w}\bigr) = x_{j,i}.
    \]
    Summing over $j$ yields the $i$th component of $2\,X^\top\!(X\,\mathbf{w} - \mathbf{y})$. 
    Hence in vector form, the gradient is $2\,X^\top\!(X\,\mathbf{w} - \mathbf{y})$.
  \end{solution}
\end{exercise}

\begin{exercise}{Existence of a Solution}
  Prove that the system of equations for least squares regression always has at least one solution, and discuss the possible number of solutions.
  
  \begin{solution}
    The normal equations are $X^\top X\,\mathbf{w} = X^\top \mathbf{y}.$ 
    Because $X^\top X$ is positive semidefinite, at least one solution always minimizes $\|X\,\mathbf{w} - \mathbf{y}\|_2.$ 
    If $X$ has full column rank, $X^\top X$ is invertible, giving a unique solution 
    $\mathbf{w}^* = (X^\top X)^{-1} X^\top \mathbf{y}.$ 
    Otherwise, infinitely many solutions satisfy the normal equations.
  \end{solution}
\end{exercise}

\end{document}