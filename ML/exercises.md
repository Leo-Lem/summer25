[[ML/exercises.pdf|exercises]]
## 0. Repeat Linear Algebra
**Norms**
- 0-norm: non-zeros
- 1-norm: sum
- 2-norm (Euclid): root of squares
- $\infty$-norm (max): maximum
**Determinant**: factor of area increase or decrease. $det(A)=ad-bc$. 3x3: diagonals - reverse diagonals.
**Trace**: sum of main diagonal. $tr(A)=a+c$.
**Inverse**: reverse of a transformation. $AA^{-1}=I$. No inverse if $det(A)=0$.
**Eigenvalue**: TODO
**Eigenvector**: TODO
**Definiteness**: TODO
**Rank**: output dimensions. linearly independent columns.
**Gaussian elimination**: TODO
**Matrix multiplication**: TODO
## 1. Least Squares Regression
TODO

## 2. Optimisation for ML
**gradient**: partial derivates.
**convex sets**: all points are connected with other points.
**convex functions**: convex set and connection is contained in shape.
**convexity rules**: linear is convex and concave. norm is convex. square is convex if $A$ is psd ($w^TAw$).
**exact line search for step length/learning rate**: if i move only along given line, how far should i move to minimise?
TODO